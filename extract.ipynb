{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PG0VOT7-0gqx"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "token = {\"username\":\"your_username\",\"key\":\"your_key\"}   # Download kaggle.json file from your kaggle account and fill in your username and key\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
    "    json.dump(token, file)\n",
    "\n",
    "!kaggle datasets download -d wcukierski/enron-email-dataset -p /content\n",
    "!unzip \\*.zip\n",
    "\n",
    "chunk = pd.read_csv('/content/emails.csv', chunksize=516000)\n",
    "df = next(chunk)\n",
    "\n",
    "def get_text(Series, row_num_slicer):\n",
    "    \"\"\"returns a Series with text sliced from a list split from each message. Row_num_slicer\n",
    "    tells function where to slice split text to find only the body of the message.\"\"\"\n",
    "    result = pd.Series(index=Series.index)\n",
    "    for row, message in enumerate(Series):\n",
    "        message_words = message.split('\\n')\n",
    "        del message_words[:row_num_slicer]\n",
    "        result.iloc[row] = message_words\n",
    "    return result\n",
    "\n",
    "def extract(st):\n",
    "  match = re.search('Subject:(.+?)Thanks',st)\n",
    "  if match:\n",
    "    st = st[match.start():match.end()]\n",
    "    return st\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "remove = string.punctuation\n",
    "remove = remove.replace(\":\", \"\") # don't remove colons\n",
    "pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "\n",
    "def rm_symbols(s):\n",
    "  s = ''.join(re.sub(pattern, \"\", s))       \n",
    "  return s\n",
    "\n",
    "def rm_slash_t(s):\n",
    "  s = ''.join(re.sub(r'\\t','',s))       \n",
    "  return s\n",
    "\n",
    "def rm_colon_word(s):\n",
    "  s = ''.join(re.sub(r'[A-Za-z0-9][A-Za-z0-9. ]*:','',s))       \n",
    "  return s\n",
    "\n",
    "def rm_colon(s):\n",
    "  s = ''.join(re.sub(r':','',s))       \n",
    "  return s\n",
    "\n",
    "def rm_emailid(s):\n",
    "  s = ''.join(re.sub(r'\\S*@\\S*\\s?','',s))       \n",
    "  return s\n",
    "\n",
    "def rm_date(s):\n",
    "  s = ''.join(re.sub(r'[0-9]{2}[\\/,:][0-9]{2}[\\/,:][0-9]{2,4}','',s))        \n",
    "  return s\n",
    "\n",
    "def rm_time(s):\n",
    "  s = ''.join(re.sub(r'(1[0-2]|0?[1-9]):([0-5][0-9]) ([AaPp][Mm])','',s))       \n",
    "  return s\n",
    "\n",
    "def rm_file_name(s):\n",
    "  s = ''.join(re.sub(r'/[^\\\\]*\\.(\\w+)$/','',s))       \n",
    "  return s\n",
    "\n",
    "def rm_digit(s):\n",
    "  s = ''.join([i for i in s if not i.isdigit()])\n",
    "  return s\n",
    "\n",
    "def rm_url(s):\n",
    "  s = ''.join(re.sub(r'http\\S+','',s))\n",
    "  return s\n",
    "\n",
    "def rm_spaces(s):\n",
    "  s = ' '.join(s.split())\n",
    "  return s\n",
    "\n",
    "def rm_slash(s):\n",
    "  s = ''.join(re.sub(r'\\S*/\\S*\\s?','',s))   \n",
    "  return s\n",
    "\n",
    "def rm_tab(s):\n",
    "  s = ''.join(re.sub(r'\\S*\\\\S*\\s?','',s))   \n",
    "  return s\n",
    "\n",
    "def remove_ent(s):\n",
    "  doc = nlp(s)\n",
    "  print(type(doc))\n",
    "  print([(X.text, X.label_) for X in doc.ents])\n",
    "  text_no_namedentities = \"\"\n",
    "  for token in doc:\n",
    "    if not token.ent_type:\n",
    "      text_no_namedentities += token.text\n",
    "      if token.whitespace_:\n",
    "        text_no_namedentities += \" \"\n",
    "  return text_no_namedentities\n",
    "\n",
    "df['text'] = get_text(df.message, 15)\n",
    "\n",
    "df = df[['text']]\n",
    "\n",
    "df['text'] = df['text'].apply(str)\n",
    "df['text'] = df['text'].apply(extract)\n",
    "\n",
    "df = df.replace(to_replace='None', value=np.nan).dropna()\n",
    "\n",
    "df['text'] = df['text'].apply(rm_slash_t)\n",
    "df['text'] = df['text'].apply(rm_emailid)\n",
    "df['text'] = df['text'].apply(rm_date)\n",
    "df['text'] = df['text'].apply(rm_time)\n",
    "df['text'] = df['text'].apply(rm_digit)\n",
    "df['text'] = df['text'].apply(rm_url)\n",
    "df['text'] = df['text'].apply(rm_symbols)\n",
    "df['text'] = df['text'].apply(rm_slash)\n",
    "df['text'] = df['text'].apply(rm_file_name)\n",
    "df['text'] = df['text'].apply(rm_tab)\n",
    "df['text'] = df['text'].apply(rm_colon_word)\n",
    "df['text'] = df['text'].apply(rm_colon)\n",
    "df['text'] = df['text'].apply(rm_spaces)\n",
    "\n",
    "df['text'] = df['text'].apply(remove_ent)\n",
    "df['text'] = df['text'].apply(remove_ent)\n",
    "df['text'] = df['text'].apply(remove_ent)\n",
    "\n",
    "\n",
    "df = df[df.text != 'Thanks']  \n",
    "df.dropna\n",
    "df.to_csv('entities_removed2.csv', index= False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "extract.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
